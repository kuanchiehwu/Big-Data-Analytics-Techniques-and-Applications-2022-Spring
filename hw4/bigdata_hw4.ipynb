{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bigdata_hw4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get -y install openjdk-8-jre-headless\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPkTqr0lWf2c",
        "outputId": "b7fb340f-65a6-4aad-8dd3-e6a8f1879465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "openjdk-8-jre-headless is already the newest version (8u312-b07-0ubuntu1~18.04).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  cuda-command-line-tools-10-0 cuda-command-line-tools-10-1\n",
            "  cuda-command-line-tools-11-0 cuda-compiler-10-0 cuda-compiler-10-1\n",
            "  cuda-compiler-11-0 cuda-cuobjdump-10-0 cuda-cuobjdump-10-1\n",
            "  cuda-cuobjdump-11-0 cuda-cupti-10-0 cuda-cupti-10-1 cuda-cupti-11-0\n",
            "  cuda-cupti-dev-11-0 cuda-demo-suite-10-0 cuda-demo-suite-10-1\n",
            "  cuda-demo-suite-11-0 cuda-demo-suite-11-1 cuda-documentation-10-0\n",
            "  cuda-documentation-10-1 cuda-documentation-11-0 cuda-documentation-11-1\n",
            "  cuda-drivers cuda-drivers-510 cuda-gdb-10-0 cuda-gdb-10-1 cuda-gdb-11-0\n",
            "  cuda-gpu-library-advisor-10-0 cuda-gpu-library-advisor-10-1\n",
            "  cuda-libraries-10-0 cuda-libraries-10-1 cuda-libraries-11-0\n",
            "  cuda-memcheck-10-0 cuda-memcheck-10-1 cuda-memcheck-11-0\n",
            "  cuda-nsight-compute-10-0 cuda-nsight-compute-10-1 cuda-nsight-compute-11-0\n",
            "  cuda-nsight-compute-11-1 cuda-nsight-systems-10-1 cuda-nsight-systems-11-0\n",
            "  cuda-nsight-systems-11-1 cuda-nvcc-10-0 cuda-nvcc-10-1 cuda-nvcc-11-0\n",
            "  cuda-nvdisasm-10-0 cuda-nvdisasm-10-1 cuda-nvdisasm-11-0 cuda-nvml-dev-10-0\n",
            "  cuda-nvml-dev-10-1 cuda-nvml-dev-11-0 cuda-nvprof-10-0 cuda-nvprof-10-1\n",
            "  cuda-nvprof-11-0 cuda-nvprune-10-0 cuda-nvprune-10-1 cuda-nvprune-11-0\n",
            "  cuda-nvtx-10-0 cuda-nvtx-10-1 cuda-nvtx-11-0 cuda-runtime-10-0\n",
            "  cuda-runtime-10-1 cuda-runtime-11-0 cuda-runtime-11-1 cuda-samples-10-0\n",
            "  cuda-samples-10-1 cuda-samples-11-0 cuda-samples-11-1 cuda-sanitizer-11-0\n",
            "  cuda-sanitizer-api-10-1 dkms freeglut3 freeglut3-dev keyboard-configuration\n",
            "  libargon2-0 libcap2 libcryptsetup12 libdevmapper1.02.1 libfontenc1 libidn11\n",
            "  libip4tc0 libjansson4 libnvidia-cfg1-510 libnvidia-common-460\n",
            "  libnvidia-common-510 libnvidia-decode-510 libnvidia-encode-510\n",
            "  libnvidia-extra-510 libnvidia-fbc1-510 libnvidia-gl-510 libpam-systemd\n",
            "  libpolkit-agent-1-0 libpolkit-backend-1-0 libpolkit-gobject-1-0 libxfont2\n",
            "  libxi-dev libxkbfile1 libxmu-dev libxmu-headers libxnvctrl0\n",
            "  nsight-compute-2020.2.0 nsight-compute-2020.2.1 nsight-systems-2019.5.2\n",
            "  nsight-systems-2020.3.2 nsight-systems-2020.3.4 nvidia-compute-utils-510\n",
            "  nvidia-dkms-510 nvidia-driver-510 nvidia-kernel-common-510\n",
            "  nvidia-kernel-source-510 nvidia-modprobe nvidia-settings nvidia-utils-510\n",
            "  policykit-1 policykit-1-gnome python3-xkit screen-resolution-extra systemd\n",
            "  systemd-sysv udev x11-xkb-utils xserver-common xserver-xorg-core-hwe-18.04\n",
            "  xserver-xorg-video-nvidia-510\n",
            "Use 'apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "\n",
        "sparkConf = SparkConf().setAppName(\"hw4\")\n",
        "sc = SparkContext(conf = sparkConf)"
      ],
      "metadata": {
        "id": "q1rd91TpRZZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "trainpath = '/content/gdrive/My Drive/bigdata/hw4/200[3-4].csv'\n",
        "testpath = '/content/gdrive/My Drive/bigdata/hw4/2005.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc5Lp27i0ofk",
        "outputId": "f3225e6d-04a0-4c46-af9c-94c960eb175e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selcol = [1, 3, 5, 7, 18, 25]\n",
        "\n",
        "def handelNa(line):\n",
        "    for col in selcol:\n",
        "        if line[col] == 'NA':\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def prepData(path):\n",
        "    rawdata = sc.textFile(path)\n",
        "    header = rawdata.first()\n",
        "\n",
        "    data = rawdata.filter(lambda line: line != header) \\\n",
        "                  .map(lambda line: line.split(',')) \\\n",
        "                  .filter(lambda line: line[21] == '0') \\\n",
        "                  .filter(lambda line: line[23] == '0') \\\n",
        "                  .filter(lambda line: handelNa(line))\n",
        "    # print(data.first())\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "OUmM2UwVvaju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "traindata = prepData(trainpath)\n",
        "print(traindata.first())\n",
        "testdata = prepData(testpath)\n",
        "print(testdata.first())"
      ],
      "metadata": {
        "id": "DFGSWrcUxDMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f91c22ff-8877-4faa-a271-a4faa378544a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2003', '6', '29', '7', '1756', '1725', '1904', '1838', 'UA', '781', 'N228UA', '128', '133', '103', '26', '31', 'DEN', 'LAX', '862', '8', '17', '0', '', '0', '0', '26', '0', '0', '0']\n",
            "['2005', '1', '28', '5', '1603', '1605', '1741', '1759', 'UA', '541', 'N935UA', '158', '174', '131', '-18', '-2', 'BOS', 'ORD', '867', '4', '23', '0', '', '0', '0', '0', '0', '0', '0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.regression import LabeledPoint\n",
        "\n",
        "def labelData(line):\n",
        "    features = []\n",
        "    for col in selcol[:-1]:\n",
        "        features.append(float(line[col]))\n",
        "\n",
        "    lbl = 0.0\n",
        "    if(float(line[25]) > 0.0):\n",
        "        lbl = 1.0\n",
        "    return LabeledPoint(lbl, features)"
      ],
      "metadata": {
        "id": "boHTNWSq-hlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainlabeldata = traindata.map(lambda line: labelData(line))\n",
        "print(trainlabeldata.first())\n",
        "testlabeldata = testdata.map(lambda line: labelData(line))\n",
        "print(testlabeldata.first())"
      ],
      "metadata": {
        "id": "mmqnu_VV-IXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d59feaaa-40a0-4634-a7c7-019ff3d5b52a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1.0,[6.0,7.0,1725.0,1838.0,862.0])\n",
            "(0.0,[1.0,5.0,1605.0,1759.0,867.0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
        "\n",
        "model = LogisticRegressionWithLBFGS.train(trainlabeldata)\n",
        "\n",
        "valtraindata, validationdata = trainlabeldata.randomSplit([0.7, 0.3])\n",
        "val_model = LogisticRegressionWithLBFGS.train(valtraindata)"
      ],
      "metadata": {
        "id": "8CHAsoXhtNS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "\n",
        "predictionAndLabels = testlabeldata.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
        "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
        "\n",
        "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
        "print(\"Area under ROC = %s\" % metrics.areaUnderROC)\n",
        "\n",
        "val_predictionAndLabels = validationdata.map(lambda lp: (float(val_model.predict(lp.features)), lp.label))\n",
        "val_metrics = BinaryClassificationMetrics(val_predictionAndLabels)\n",
        "\n",
        "print(\"Area under PR = %s\" % val_metrics.areaUnderPR)\n",
        "print(\"Area under ROC = %s\" % val_metrics.areaUnderROC)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOh7pvnjtk_p",
        "outputId": "6c47996e-0932-4ae9-c8f7-e7e3aa4056c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area under PR = 0.015928868937046734\n",
            "Area under ROC = 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area under PR = 0.014896737991571398\n",
            "Area under ROC = 0.5\n"
          ]
        }
      ]
    }
  ]
}